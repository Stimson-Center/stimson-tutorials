{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 0:\n",
    "Load Text document to be statistical parsing/tagging from your current directory\n",
    "\n",
    "Parse, scrub / cleanse and tag uploaded Text document\n",
    "\n",
    "https://github.com/DerwenAI/pytextrank\n",
    "https://github.com/DerwenAI/pytextrank/blob/master/example.ipynb\n",
    "https://www.thinkinfi.com/2018/09/automatic-keyword-extraction-using_30.html\n",
    "https://medium.com/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5\n",
    "https://xang1234.github.io/textrank/\n",
    "\n",
    "Attribution\n",
    "PyTextRank has an MIT license, which is succinct and simplifies use in commercial applications.\n",
    "\n",
    "Please use the following BibTeX entry for citing PyTextRank in publications:\n",
    "\n",
    "@Misc{PyTextRank,\n",
    "author = {Nathan, Paco},\n",
    "title = {PyTextRank, a Python implementation of TextRank for phrase extraction and summarization of text documents},\n",
    "    howpublished = {\\url{https://github.com/DerwenAI/pytextrank/}},\n",
    "    year = {2016}\n",
    "    }\n",
    "\n",
    "INPUTS: Text doc for the text input  \n",
    "OUTPUT: JSON format of the original text document, scrubed and put into id then text values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display\n",
    "import spacy\n",
    "import pytextrank\n",
    "import unicodedata\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "DEBUG = True # False # True\n",
    "\n",
    "\n",
    "def cleanup_text (text):\n",
    "    \"\"\"\n",
    "    It scrubs the garbled from its stream...\n",
    "    Or it gets the debugger again.\n",
    "    \"\"\"\n",
    "    x = \" \".join(map(lambda s: s.strip(), text.split(\"\\n\"))).strip()\n",
    "\n",
    "    x = x.replace('“', '\"').replace('”', '\"')\n",
    "    x = x.replace(\"‘\", \"'\").replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    x = x.replace('…', '...').replace('–', '-')\n",
    "\n",
    "    x = str(unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "    # some content returns text in bytes rather than as a str ?\n",
    "    try:\n",
    "        assert type(x).__name__ == 'str'\n",
    "    except AssertionError:\n",
    "        print(\"not a string?\") # , type(line), line)\n",
    "\n",
    "    return x\n",
    "\n",
    "def pretty_print (obj, indent=False):\n",
    "    \"\"\"\n",
    "    pretty print a JSON object\n",
    "    \"\"\"\n",
    "\n",
    "    if indent:\n",
    "        return json.dumps(obj, sort_keys=True, indent=2, separators=(',', ': '))\n",
    "    else:\n",
    "        return json.dumps(obj, sort_keys=True)\n",
    "\n",
    "# https://gist.github.com/BrambleXu/3d47bbdbd1ee4e6fc695b0ddb88cbf99\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "\n",
    "    def set_stopwords(self, stopwords):\n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = self.nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "\n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "\n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "\n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "\n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "\n",
    "        return g_norm\n",
    "\n",
    "\n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        keywords = dict()\n",
    "        for i, (k, v) in enumerate(node_weight.items()):\n",
    "            keywords[k] = v\n",
    "            if i > number:\n",
    "                break\n",
    "        return keywords\n",
    "\n",
    "    def analyze(self, text,\n",
    "                candidate_pos=['NOUN', 'PROPN'],\n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "\n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "\n",
    "        # Pare text by spaCy\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "\n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "\n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "\n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "\n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "\n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dirname = os.getcwd()\n",
    "input_path_stage1 = os.path.abspath(os.path.join(dirname, \"..\", \"data\", \"textrank.txt\"))\n",
    "with open(input_path_stage1, 'r') as f1:\n",
    "    content = f1.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the strongest phrases in the text file's contents:\n",
    "\n",
    "https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank\tCount\tPhrase\n",
      "\n",
      "0.0651\t    5\tillegal fishing vessels\n",
      "0.0640\t    1\tillegal chinese fishing vessels\n",
      "0.0631\t    5\tillegal fishing boats\n",
      "0.0628\t    3\tillegal foreign fishing vessels\n",
      "0.0622\t    1\tillegal chinese fishing boats\n",
      "0.0614\t    2\tsuspected illegal fishing vessels\n",
      "0.0612\t   26\tfishing vessels\n",
      "0.0609\t    1\tchina’s illegal fishing\n",
      "0.0608\t    1\tillegal fishing vessel viking\n",
      "0.0605\t   10\tchinese fishing vessels\n"
     ]
    }
   ],
   "source": [
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "tr = pytextrank.TextRank()\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "\n",
    "# *************************************************************\n",
    "# THIS STEP CAN TAKE A MINUTE OR TWO!\n",
    "doc = nlp(content)\n",
    "# *************************************************************\n",
    "\n",
    "output_textrank_phrases = os.path.abspath(os.path.join(dirname, \"..\", \"textrank_phrases.tsv\"))\n",
    "# examine the top-ranked phrases in the document\n",
    "with open(output_textrank_phrases, 'w') as f:\n",
    "    print(\"Rank\\tCount\\tPhrase\\n\")\n",
    "    f.write(\"Rank\\tCount\\tPhrase\\n\")\n",
    "    i = 0\n",
    "    for p in doc._.phrases:\n",
    "        if i < 10:\n",
    "            print(\"{:.4f}\\t{:5d}\\t{}\".format(p.rank, p.count, p.text))\n",
    "        f.write(\"{:.4f}\\t{:5d}\\t{}\\n\".format(p.rank, p.count, p.text))\n",
    "        i += 1\n",
    "        # print(p.chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the strongest keywords in the text file's contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank\t\t\tKeyword\n",
      "50.85208112995197\tfishing\n",
      "26.690703891139275\tfishermen\n",
      "25.933127678317764\twaters\n",
      "24.640941831954592\tvessels\n",
      "20.62912721198954\tvessel\n",
      "19.78853589003089\tsea\n",
      "19.75132970907416\tpolice\n",
      "19.272495625592633\tcrew\n",
      "18.890463809815717\tboats\n",
      "18.717244583256363\tboat\n"
     ]
    }
   ],
   "source": [
    "output_textrank_keywords = os.path.abspath(os.path.join(dirname, \"..\", \"textrank_keywords.tsv\"))\n",
    "\n",
    "tr4w = TextRank4Keyword(nlp)\n",
    "tr4w.analyze(content, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "with open(output_textrank_keywords, 'w') as f:\n",
    "    keywords = tr4w.get_keywords(100)\n",
    "    print(\"Rank\\t\\t\\tKeyword\")\n",
    "    f.write(\"Rank\\tKeyword\\n\")\n",
    "    for i, (k, v) in enumerate(keywords.items()):\n",
    "        if i < 10:\n",
    "            print(f\"{v}\\t{k}\")\n",
    "        f.write(f\"{v}\\t{k}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}